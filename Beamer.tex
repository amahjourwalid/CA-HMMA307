\documentclass[unknownkeysallowed]{beamer}
\usepackage[french,english]{babel}
\input{Beamer_js}
\input{shortcuts_js}
%\usepackage{./OrganizationFiles/tex/sty/shortcuts_js}
\usepackage{csquotes}

\graphicspath{{./images/}}

\addbibresource{Bibliographie.bib}
\usepackage{enumerate}
\usepackage{amsmath}
\begin{document}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%             Headers               %%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}[noframenumbering]
		\thispagestyle{empty}
		\bigskip
		\bigskip
		\begin{center}{
				\LARGE\color{marron}
				\textbf{HMMA 307 : Advanced Linear Modeling}
				\textbf{ }\\
				\vspace{0.5cm}
			}
			
			\color{marron}
			\textbf{Chapter 5 : Random Effects Models}
		\end{center}
		
		\vspace{0.5cm}
		
		\begin{center}
			\textbf{ISKOUNEN SELENA \ AMAHJOUR WALID \ RUDOLF RÖMISCH } \\
			\vspace{0.1cm}
			\url{https://github.com/selenaiskounen/CA-HMMA307}\\
			\vspace{0.5cm}
			Université de Montpellier \\
		\end{center}
		
		\centering
		\includegraphics[width=0.13\textwidth]{Logo.pdf}
	\end{frame}
	
	\begin{frame}{Overview} 
		\tableofcontents
	\end{frame}
	
	\section{Random Effect Models}
	\begin{frame}{Motivation}
		We consider cases where we get random samples from large populations. 
		Since the goal is to make statements about properties of whole populations and not about observed individuals, it is rather natural to assume random samples. 
		Now we elaborate an example with machines. We assume that we assess the quality of produced samples from some machines.
		
	\end{frame}
	
	\subsection{One-Way ANOVA}
	\begin{frame}{One-Way ANOVA: Model}
		Our model:
		
		$$
		Y_{ij} = \mu + \alpha_i + \epsilon_{ij}
		$$
		Here we have the following variables and assumptions:
		\begin{itemize}
			\item $Y_{ij}$ as the quality of the j-th sample on the i-th machine 
			\item $\mu$ as the global mean
			\item $\alpha_i$ i.i.d.  
			$\sim \mathcal{N}(0,\sigma^2_{\alpha})$ as the effect of the i-th machine
			\item $\epsilon_{ij}$ i.i.d. $\sim \mathcal{N}(0,\sigma^2)$ as the error term for the j-th sample and the i-th machine
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Specialty}
		The model has strong similarities with the fixed models. But the $\alpha_i's$ are random variables and not fixed unkown parameters.
		With that change the properties of the model will be strongly influenced.
		\begin{itemize}
			\item We get the new parameter $\sigma^2_{\alpha}$
			\item Our model is also called variance components models since we have now two different variances $\sigma^2_{\alpha}$ and $\sigma^2$
		\end{itemize}
	\end{frame}
	
	\begin{frame}{Properties}
		We consider some properties for our model:
		\begin{itemize}
			\item The expected value of $Y_{ij}$ is $E[Y_{ij}]=\mu$, since $E[Y_{ij}] = E[\mu + \alpha_i + \epsilon_{ij}] = E[\mu] + E[\alpha_i] + E[\epsilon_{ij}] = \mu$
			\item The variance is $Var(Y_{ij}) = \sigma^2_{\alpha} +\sigma^2$ since
			
			$Var(Y_{ij}) = E[Y_{ij}^2] - E[Y_{ij}]^2
			= E[\mu^2] + E[\mu \alpha_i] + E[\mu \epsilon_{ij}] + E[\alpha_i \mu] + E[\alpha_i \alpha_i] + E[\alpha_i \epsilon_{ij}] + E[\epsilon_{ij} \mu] + E[\epsilon_{ij}\alpha_i] + E[\epsilon_{ij}\epsilon_{ij}] - \mu^2
			= \mu^2 + \mu \underbrace{E[\alpha_i]}_\text{= 0} + \mu \underbrace{E[\epsilon_{ij}]}_\text{= 0} + \underbrace{E[\alpha_i]}_\text{=0}\mu + E[\alpha_i^2] + \underbrace{E[\alpha_i]}_\text{= 0}\underbrace{E[\epsilon_{ij}]}_\text{= 0} + \underbrace{E[\epsilon_{ij}]}_\text{=0}\mu + \underbrace{E[\epsilon_{ij}]}_\text{=0} \underbrace{E[\alpha_i]}_\text{=0} + E[\epsilon_{ij}^2] - \mu^2 
			= E[\alpha_i^2] + E[\epsilon_{ij}^2]
			= \sigma^2_{\alpha} + \sigma^2$
		\end{itemize}
	\end{frame}
	
	\begin{frame}
		For the correlation structure we get:
		\begin{equation*}
		D_{it} =
		\begin{cases}
		\quad 0 & i \neq k\\
		\sigma^2_{\alpha}/(\sigma^2_{\alpha} + \sigma^2) & i = k, j\neq l\\
		\quad 1 & i=k, j=l
		\end{cases}
		\end{equation*}
		
		Here we can see that observations from different machines are uncorrelated (here independent) while observations from the same machine are correlated. \\
		$\sigma^2_{\alpha}/(\sigma^2_{\alpha} + \sigma^2)$ is also called the intraclass correlation (ICC). For $\sigma^2_{\alpha} >> \sigma^2$, the ICC gets "large" what means that observations from the same "group" are very similar to each other.
		
		So values "sharing" the same $"\alpha_i "$ are correlated, while in the fixed effects model all values would be independent (because there the $\alpha_i$ are parameters, that is fixed, unknown quantities).
	\end{frame}
	
	\begin{frame}
		Parameter estimation for the variance components $\sigma^2_{\alpha}$ and $\sigma^2$ is typically being done with a technique called restricted maximum likelihood (REML).\\
		It is also possible to use the "classical" maximum-likelihood estimator but REML estimates are less biased. The parameter $\mu$ is estimated with maximum-likelihood assuming that the variances are known. 
	\end{frame}
	\subsection{More than one Factor}
	\begin{frame}
		Now we can extend our model to the two-way ANOVA situation.\\
		As an example we have a manufacturer who is developing a new spectrophometer for medical labs. A critical issue is consistency of measurements from day to day among different machines. The response is the triglyceride level [mg/dl] of a sample.
	\end{frame}
	
	\begin{frame}{More than One Factor: Model}
		We use the model:
		\begin{equation*}
		Y_{ijk} = \mu + \alpha_i + \beta_j + (\alpha \beta)_{ij} + \epsilon_{ij},
		\end{equation*}
		for the random effects we have the usual assumptions
		\begin{itemize}
			\item $Y_{ijk}$ is the triglyceride level of the k-th sample on day i and machine j,
			\item $\alpha_i$ i.i.d.  
			$\sim \mathcal{N}(0,\sigma^2_{\alpha})$ is the random effect of day,
			\item $\beta_j$ i.i.d.  
			$\sim \mathcal{N}(0,\sigma^2_{\beta})$ is the random effect of machine,
			\item $(\alpha \beta)_{ij}$ i.i.d.  
			$\sim \mathcal{N}(0,\sigma^2_{\alpha \beta})$ random interaction term between day and machine.
		\end{itemize}
	\end{frame}
	
	\subsection{Nesting}
	\begin{frame}{Motivation}
		Now we introduce a new concept. We consider the strength of a chemical paste product which was measured for a total of sixty samples coming from ten randomly selected delivery batches each containing three randomly selected casks. Now we can find a new way of combining factors. Cask one in batch 1 has nothing to do with cask one in batch two and so on. The "1" of cask has a different meaning for every batch. Hence, cask and batch are not crossed. We say cask is nested in batch.\\
		The battch effect does not seem to be very pronounced. On the other side, casks within the same batch can be substantially different. Let us now model this with appropriate random effects.
	\end{frame}
	
	
	\begin{frame}{Nesting: Model}
		We can use the model:
		\begin{equation*}
		Y_{ijk} = \mu + \alpha_i + \beta_{j(i)} + \epsilon_{k(ij)}.
		\end{equation*}
		\begin{itemize}
			\item $Y_{ijk}$ is the strength of the k-th sample of cask j in batch i,
			\item $\alpha_i$ i.i.d.  
			$\sim \mathcal{N}(0,\sigma^2_{\alpha})$ is the (random) effect of batch,
			\item $\beta_{j(i)}$ i.i.d.  
			$\sim \mathcal{N}(0,\sigma^2_{\beta})$ is the (random) effect of cask within batch,
			\item $\epsilon_{k(ij)}$ i.i.d.  
			$\sim \mathcal{N}(0,\sigma^2)$ is the "usual" error term.
		\end{itemize}
		Here we can see the special notation for $\beta_{j(i)}$ and $\epsilon_{k(ij)}$ which emphasizes that cask is nested in batch.
	\end{frame}
	
	\section{Mixed Effects Models}
	\begin{frame}
		
	\end{frame}
	
	
	
	
\end{document}